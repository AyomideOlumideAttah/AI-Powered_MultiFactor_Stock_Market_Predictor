{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "393J-D5Qdhfh",
        "outputId": "8a532f50-8ba6-4fef-d111-2db3828307b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fredapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MK8TyHEdfFr",
        "outputId": "785aeb44-f225-430c-fa05-efe1368fcf6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fredapi in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fredapi) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fredapi) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->fredapi) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4SW1wds_QW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the necessary libraries"
      ],
      "metadata": {
        "id": "maISXWtF_NF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Clearing any warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning imports\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn import model_selection\n",
        "\n",
        "# FRED API for economic data\n",
        "from fredapi import Fred"
      ],
      "metadata": {
        "id": "2I1ZiuJa_Mpf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The StockMarketPredictor Class (for training and testing the models)"
      ],
      "metadata": {
        "id": "pf4Ik6TG_gP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "g7bktKzUcMv2"
      },
      "outputs": [],
      "source": [
        "class StockMarketPredictor:\n",
        "    def __init__(self, fred_api_key, stock_symbol='SPY', start_date='2010-01-01'):\n",
        "        \"\"\"\n",
        "        Initialize the Stock Market Predictor\n",
        "\n",
        "        Parameters:\n",
        "        fred_api_key (str)\n",
        "        stock_symbol (str)\n",
        "        start_date (str)\n",
        "        \"\"\"\n",
        "        self.fred_api_key = fred_api_key\n",
        "        self.stock_symbol = stock_symbol\n",
        "        self.start_date = start_date\n",
        "        self.end_date = datetime.now().strftime('%Y-%m-%d')\n",
        "        self.data = None\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "\n",
        "    def collect_stock_data(self):\n",
        "        \"\"\"Collect stock data using yfinance\"\"\"\n",
        "        print(f\"Collecting stock data for {self.stock_symbol}...\")\n",
        "        stock = yf.Ticker(self.stock_symbol)\n",
        "        stock_data = stock.history(start=self.start_date, end=self.end_date)\n",
        "\n",
        "        # Remove timezone information to match FRED data\n",
        "        stock_data.index = stock_data.index.tz_localize(None)\n",
        "\n",
        "        # Calculate the technical indicators\n",
        "        stock_data['Returns'] = stock_data['Close'].pct_change()\n",
        "        stock_data['Log_Returns'] = np.log(stock_data['Close'] / stock_data['Close'].shift(1))\n",
        "        stock_data['Volatility'] = stock_data['Returns'].rolling(window=21).std()\n",
        "        stock_data['MA_5'] = stock_data['Close'].rolling(window=5).mean()\n",
        "        stock_data['MA_20'] = stock_data['Close'].rolling(window=20).mean()\n",
        "        stock_data['RSI'] = self.calculate_rsi(stock_data['Close'])\n",
        "\n",
        "        # Create target variables\n",
        "        stock_data['Next_Day_Return'] = stock_data['Returns'].shift(-1)\n",
        "        stock_data['Direction'] = (stock_data['Next_Day_Return'] > 0).astype(int)\n",
        "\n",
        "        return stock_data[['Close', 'Volume', 'Returns', 'Log_Returns', 'Volatility',\n",
        "                          'MA_5', 'MA_20', 'RSI', 'Next_Day_Return', 'Direction']]\n",
        "\n",
        "    def calculate_rsi(self, prices, window=14):\n",
        "        \"\"\"Calculate Relative Strength Index\"\"\"\n",
        "        delta = prices.diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "        rs = gain / loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        return rsi\n",
        "\n",
        "    def collect_economic_data(self):\n",
        "        \"\"\"Collect macroeconomic data from FRED\"\"\"\n",
        "        print(\"Collecting macroeconomic data from FRED...\")\n",
        "\n",
        "        # Initialize FRED API\n",
        "        fred = Fred(api_key=self.fred_api_key)\n",
        "\n",
        "        # Economic indicators to collect\n",
        "        indicators = {\n",
        "            'GDP': 'GDP',                           # Gross Domestic Product\n",
        "            'UNEMPLOYMENT': 'UNRATE',               # Unemployment Rate\n",
        "            'INFLATION': 'CPIAUCSL',                # Consumer Price Index\n",
        "            'FED_RATE': 'FEDFUNDS',                 # Federal Funds Rate\n",
        "            'VIX': 'VIXCLS',                        # VIX Volatility Index\n",
        "            '10Y_TREASURY': 'GS10',                 # 10-Year Treasury Rate\n",
        "            '3M_TREASURY': 'GS3M',                  # 3-Month Treasury Rate\n",
        "            'CONSUMER_SENTIMENT': 'UMCSENT',        # Consumer Sentiment\n",
        "            'INDUSTRIAL_PRODUCTION': 'INDPRO',      # Industrial Production Index\n",
        "            'HOUSING_STARTS': 'HOUST',              # Housing Starts\n",
        "            'RETAIL_SALES': 'RSAFS',                # Retail Sales\n",
        "            'M2_MONEY_SUPPLY': 'M2SL'               # M2 Money Supply\n",
        "        }\n",
        "\n",
        "        economic_data = pd.DataFrame()\n",
        "\n",
        "        for name, code in indicators.items():\n",
        "            try:\n",
        "                print(f\"  Fetching {name}...\")\n",
        "                series = fred.get_series(code, start=self.start_date, end=self.end_date)\n",
        "\n",
        "                # Ensure timezone-naive index\n",
        "                if hasattr(series.index, 'tz') and series.index.tz is not None:\n",
        "                    series.index = series.index.tz_localize(None)\n",
        "\n",
        "                series = pd.DataFrame({name: series})\n",
        "\n",
        "                if economic_data.empty:\n",
        "                    economic_data = series\n",
        "                else:\n",
        "                    economic_data = economic_data.join(series, how='outer')\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Could not fetch {name}: {e}\")\n",
        "\n",
        "        # Calculate derived features\n",
        "        if 'INFLATION' in economic_data.columns:\n",
        "            economic_data['INFLATION_RATE'] = economic_data['INFLATION'].pct_change(12) * 100\n",
        "\n",
        "        if '10Y_TREASURY' in economic_data.columns and '3M_TREASURY' in economic_data.columns:\n",
        "            economic_data['YIELD_CURVE'] = economic_data['10Y_TREASURY'] - economic_data['3M_TREASURY']\n",
        "\n",
        "        return economic_data\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Combine and prepare all data for modeling\"\"\"\n",
        "        print(\"Preparing and cleaning data...\")\n",
        "\n",
        "        # Collect data\n",
        "        stock_data = self.collect_stock_data()\n",
        "        economic_data = self.collect_economic_data()\n",
        "\n",
        "        # Resample economic data to daily frequency (forward fill)\n",
        "        economic_data_daily = economic_data.resample('D').ffill()\n",
        "\n",
        "        # Combine datasets\n",
        "        combined_data = stock_data.join(economic_data_daily, how='inner')\n",
        "\n",
        "        # Handle missing values\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        feature_columns = [col for col in combined_data.columns if col not in ['Next_Day_Return', 'Direction']]\n",
        "        combined_data[feature_columns] = imputer.fit_transform(combined_data[feature_columns])\n",
        "\n",
        "        # Remove rows with missing target variables\n",
        "        combined_data = combined_data.dropna(subset=['Next_Day_Return', 'Direction'])\n",
        "\n",
        "        # Add lag features for economic indicators\n",
        "        econ_columns = [col for col in combined_data.columns if col not in stock_data.columns]\n",
        "        for col in econ_columns:\n",
        "            if col in combined_data.columns:\n",
        "                combined_data[f'{col}_lag1'] = combined_data[col].shift(1)\n",
        "                combined_data[f'{col}_lag7'] = combined_data[col].shift(7)\n",
        "\n",
        "        # Remove rows with NaN after creating lag features\n",
        "        combined_data = combined_data.dropna()\n",
        "\n",
        "        self.data = combined_data\n",
        "        print(f\"Final dataset shape: {combined_data.shape}\")\n",
        "        return combined_data\n",
        "\n",
        "    def visualize_data(self):\n",
        "        \"\"\"Create visualizations of the data\"\"\"\n",
        "        print(\"Creating visualizations...\")\n",
        "\n",
        "        # Set up the plotting style with better formatting\n",
        "        plt.style.use('default')\n",
        "        fig = plt.figure(figsize=(24, 18))\n",
        "        plt.rcParams.update({'font.size': 10})\n",
        "\n",
        "        # 1. Stock price and moving averages\n",
        "        plt.subplot(3, 3, 1)\n",
        "        plt.plot(self.data.index, self.data['Close'], label='Close Price', alpha=0.7, linewidth=1.5)\n",
        "        plt.plot(self.data.index, self.data['MA_5'], label='5-day MA', alpha=0.8, linewidth=1)\n",
        "        plt.plot(self.data.index, self.data['MA_20'], label='20-day MA', alpha=0.8, linewidth=1)\n",
        "        plt.title(f'{self.stock_symbol} Price and Moving Averages', fontsize=12, fontweight='bold')\n",
        "        plt.legend(loc='upper left', fontsize=9)\n",
        "        plt.gca().xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "        plt.xticks(rotation=45, fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Returns distribution\n",
        "        plt.subplot(3, 3, 2)\n",
        "        plt.hist(self.data['Returns'].dropna(), bins=50, alpha=0.7, edgecolor='black', color='skyblue')\n",
        "        plt.title('Daily Returns Distribution', fontsize=12, fontweight='bold')\n",
        "        plt.xlabel('Daily Returns', fontsize=10)\n",
        "        plt.ylabel('Frequency', fontsize=10)\n",
        "        plt.xticks(fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Volatility over time\n",
        "        plt.subplot(3, 3, 3)\n",
        "        plt.plot(self.data.index, self.data['Volatility'], color='orange', alpha=0.7)\n",
        "        plt.title('Volatility Over Time', fontsize=12, fontweight='bold')\n",
        "        plt.gca().xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "        plt.xticks(rotation=45, fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Economic indicators\n",
        "        plt.subplot(3, 3, 4)\n",
        "        if 'FED_RATE' in self.data.columns:\n",
        "            plt.plot(self.data.index, self.data['FED_RATE'], label='Fed Rate', linewidth=1.5)\n",
        "        if 'UNEMPLOYMENT' in self.data.columns:\n",
        "            plt.plot(self.data.index, self.data['UNEMPLOYMENT'], label='Unemployment', linewidth=1.5)\n",
        "        plt.title('Key Economic Indicators', fontsize=12, fontweight='bold')\n",
        "        plt.legend(loc='upper right', fontsize=9)\n",
        "        plt.gca().xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "        plt.xticks(rotation=45, fontsize=8)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5. Yield curve\n",
        "        plt.subplot(3, 3, 5)\n",
        "        if 'YIELD_CURVE' in self.data.columns:\n",
        "            plt.plot(self.data.index, self.data['YIELD_CURVE'], color='purple', alpha=0.7)\n",
        "            plt.title('Yield Curve (10Y - 3M)', fontsize=12, fontweight='bold')\n",
        "            plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "            plt.gca().xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "            plt.xticks(rotation=45, fontsize=8)\n",
        "            plt.yticks(fontsize=8)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 6. VIX\n",
        "        plt.subplot(3, 3, 6)\n",
        "        if 'VIX' in self.data.columns:\n",
        "            plt.plot(self.data.index, self.data['VIX'], color='red', alpha=0.7)\n",
        "            plt.title('VIX (Fear Index)', fontsize=12, fontweight='bold')\n",
        "            plt.axhline(y=20, color='orange', linestyle='--', alpha=0.5, label='High Fear')\n",
        "            plt.gca().xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "            plt.xticks(rotation=45, fontsize=8)\n",
        "            plt.yticks(fontsize=8)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 7. Correlation heatmap\n",
        "        plt.subplot(3, 3, 7)\n",
        "        correlation_data = self.data.select_dtypes(include=[np.number]).corr()\n",
        "        # Select most important correlations\n",
        "        important_features = ['Returns', 'FED_RATE', 'UNEMPLOYMENT', 'VIX', 'YIELD_CURVE', 'INFLATION_RATE']\n",
        "        available_features = [f for f in important_features if f in correlation_data.columns]\n",
        "        if len(available_features) > 1:\n",
        "            corr_subset = correlation_data.loc[available_features, available_features]\n",
        "            # Create shorter labels for better display\n",
        "            short_labels = []\n",
        "            for feature in available_features:\n",
        "                if feature == 'UNEMPLOYMENT': short_labels.append('UNEMP')\n",
        "                elif feature == 'INFLATION_RATE': short_labels.append('INFL')\n",
        "                elif feature == 'YIELD_CURVE': short_labels.append('Y_CURVE')\n",
        "                elif feature == 'FED_RATE': short_labels.append('FED')\n",
        "                else: short_labels.append(feature)\n",
        "\n",
        "            corr_subset.index = short_labels\n",
        "            corr_subset.columns = short_labels\n",
        "            sns.heatmap(corr_subset, annot=True, cmap='coolwarm', center=0, fmt='.2f',\n",
        "                       cbar_kws={'shrink': 0.8}, square=True)\n",
        "            plt.title('Feature Correlations', fontsize=12, fontweight='bold')\n",
        "            plt.xticks(fontsize=8)\n",
        "            plt.yticks(fontsize=8)\n",
        "\n",
        "        # 8. Direction distribution\n",
        "        plt.subplot(3, 3, 8)\n",
        "        direction_counts = self.data['Direction'].value_counts()\n",
        "        bars = plt.bar(['Down (0)', 'Up (1)'], direction_counts.values,\n",
        "                      color=['lightcoral', 'lightgreen'], alpha=0.8, edgecolor='black')\n",
        "        plt.title('Direction Distribution', fontsize=12, fontweight='bold')\n",
        "        plt.ylabel('Frequency', fontsize=10)\n",
        "        plt.xticks(fontsize=10)\n",
        "        plt.yticks(fontsize=8)\n",
        "        plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                    f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        # 9. Returns vs VIX\n",
        "        plt.subplot(3, 3, 9)\n",
        "        if 'VIX' in self.data.columns:\n",
        "            plt.scatter(self.data['VIX'], self.data['Returns'], alpha=0.3, s=8)\n",
        "            plt.xlabel('VIX', fontsize=10)\n",
        "            plt.ylabel('Returns', fontsize=10)\n",
        "            plt.title('Returns vs VIX', fontsize=12, fontweight='bold')\n",
        "            plt.xticks(fontsize=8)\n",
        "            plt.yticks(fontsize=8)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Improve overall layout\n",
        "        plt.tight_layout(pad=3.0)\n",
        "        plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    def prepare_features(self):\n",
        "        \"\"\"Prepare features for modeling\"\"\"\n",
        "        # Exclude target variables and non-predictive features\n",
        "        exclude_columns = ['Next_Day_Return', 'Direction', 'Close']\n",
        "        feature_columns = [col for col in self.data.columns if col not in exclude_columns]\n",
        "\n",
        "        X = self.data[feature_columns]\n",
        "        y_reg = self.data['Next_Day_Return']  # For regression\n",
        "        y_clf = self.data['Direction']        # For classification\n",
        "\n",
        "        return X, y_reg, y_clf, feature_columns\n",
        "\n",
        "    def train_classification_model(self, X, y):\n",
        "        \"\"\"Train classification model to predict market direction\"\"\"\n",
        "        print(\"Training classification models...\")\n",
        "\n",
        "        # Split data maintaining temporal order\n",
        "        split_idx = int(len(X) * 0.8)\n",
        "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        self.scalers['classification'] = scaler\n",
        "\n",
        "        # Train Classifiers\n",
        "        rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "        lr_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "        # Added cross_validation to imporve accuracy\n",
        "        seed, scoring = 17, \"accuracy\"\n",
        "\n",
        "        kfold = model_selection.KFold(n_splits=8, random_state=seed, shuffle=True)\n",
        "        rf_results = model_selection.cross_val_score(rf_clf, X_train_scaled, y_train, cv=kfold, scoring=scoring)\n",
        "        lr_results = model_selection.cross_val_score(lr_clf, X_train_scaled, y_train, cv=kfold, scoring=scoring)\n",
        "\n",
        "        rf_score, rf_std = rf_results.mean(), rf_results.std()\n",
        "        lr_score, lr_std = lr_results.mean(), lr_results.std()\n",
        "\n",
        "        print(f\"Random Forest Accuracy: {rf_score:.4f}\")\n",
        "        print(f\"Logistic Regression Accuracy: {lr_score:.4f}\")\n",
        "\n",
        "        score = max(rf_score, lr_score)\n",
        "\n",
        "        best_model, model_name = (rf_clf, \"Random Forest\") if rf_score > lr_score or rf_score == lr_score and rf_std < lr_std else (lr_clf, \"Logistic Regression\")\n",
        "        best_model.fit(X_train_scaled, y_train)\n",
        "        best_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "        self.models['classification'] = {\n",
        "            'model': best_model,\n",
        "            'name': model_name,\n",
        "            'score': score\n",
        "        }\n",
        "\n",
        "        print(f\"\\nBest Classification Model: {model_name}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, best_pred))\n",
        "\n",
        "        return X_test, y_test, best_pred\n",
        "\n",
        "    def train_regression_model(self, X, y):\n",
        "        \"\"\"Train regression model to predict returns\"\"\"\n",
        "        print(\"\\nTraining regression models...\")\n",
        "\n",
        "        # Split data maintaining temporal order\n",
        "        split_idx = int(len(X) * 0.8)\n",
        "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        self.scalers['regression'] = scaler\n",
        "\n",
        "        # Train classifiers\n",
        "        rf_reg = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
        "        lr_reg = LinearRegression()\n",
        "\n",
        "        seed, scoring = 10, \"r2\"\n",
        "\n",
        "        kfold = model_selection.KFold(n_splits=8, random_state=seed, shuffle=True)\n",
        "        rf_results = model_selection.cross_val_score(rf_reg, X_train_scaled, y_train, cv=kfold, scoring=scoring)\n",
        "        lr_results = model_selection.cross_val_score(lr_reg, X_train_scaled, y_train, cv=kfold, scoring=scoring)\n",
        "\n",
        "        rf_r2, rf_std = rf_results.mean(), rf_results.std()\n",
        "        lr_r2, lr_std = lr_results.mean(), lr_results.std()\n",
        "\n",
        "        rf_reg.fit(X_train_scaled, y_train)\n",
        "        lr_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Evaluate models\n",
        "        rf_pred = rf_reg.predict(X_test_scaled)\n",
        "        lr_pred = lr_reg.predict(X_test_scaled)\n",
        "\n",
        "        rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "        lr_mse = mean_squared_error(y_test, lr_pred)\n",
        "\n",
        "        print(f\"Random Forest - R²: {rf_r2:.4f}, MSE: {rf_mse:.6f}\")\n",
        "        print(f\"Linear Regression - R²: {lr_r2:.4f}, MSE: {lr_mse:.6f}\")\n",
        "\n",
        "        best_r2 = max(rf_r2, lr_r2)\n",
        "\n",
        "        best_model, model_name, best_pred = (rf_reg, \"Random Forest\", rf_pred) if rf_r2 > lr_r2 or rf_r2 == lr_r2 and rf_std < lr_std else (lr_reg, \"Logistic Regression\", lr_pred)\n",
        "        best_model.fit(X_train_scaled, y_train)\n",
        "        best_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "        self.models['regression'] = {\n",
        "            'model': best_model,\n",
        "            'name': model_name,\n",
        "            'r2': best_r2\n",
        "        }\n",
        "\n",
        "        print(f\"\\nBest Regression Model: {model_name}\")\n",
        "\n",
        "        return X_test, y_test, best_pred\n",
        "\n",
        "    def feature_importance(self, feature_columns):\n",
        "        \"\"\"Display feature importance\"\"\"\n",
        "        print(\"\\nFeature Importance Analysis:\")\n",
        "\n",
        "        # Classification feature importance\n",
        "        if hasattr(self.models['classification']['model'], 'feature_importances_'):\n",
        "            clf_importance = pd.DataFrame({\n",
        "                'feature': feature_columns,\n",
        "                'importance': self.models['classification']['model'].feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            print(f\"\\nTop 10 features for {self.models['classification']['name']} (Classification):\")\n",
        "            print(clf_importance.head(10))\n",
        "\n",
        "        # Regression feature importance\n",
        "        if hasattr(self.models['regression']['model'], 'feature_importances_'):\n",
        "            reg_importance = pd.DataFrame({\n",
        "                'feature': feature_columns,\n",
        "                'importance': self.models['regression']['model'].feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            print(f\"\\nTop 10 features for {self.models['regression']['name']} (Regression):\")\n",
        "            print(reg_importance.head(10))\n",
        "\n",
        "    def make_predictions(self, days_ahead=5):\n",
        "        \"\"\"Make future predictions\"\"\"\n",
        "        print(f\"\\nMaking predictions for the next {days_ahead} days...\")\n",
        "\n",
        "        # Get the latest data\n",
        "        latest_features = self.data.iloc[-1][self.feature_columns].values.reshape(1, -1)\n",
        "\n",
        "        # Scale the features\n",
        "        clf_features_scaled = self.scalers['classification'].transform(latest_features)\n",
        "        reg_features_scaled = self.scalers['regression'].transform(latest_features)\n",
        "\n",
        "        # Make predictions\n",
        "        direction_pred = self.models['classification']['model'].predict(clf_features_scaled)[0]\n",
        "        return_pred = self.models['regression']['model'].predict(reg_features_scaled)[0]\n",
        "\n",
        "        direction_prob = None\n",
        "        if hasattr(self.models['classification']['model'], 'predict_proba'):\n",
        "            direction_prob = self.models['classification']['model'].predict_proba(clf_features_scaled)[0]\n",
        "\n",
        "        print(f\"Next day direction prediction: {'UP' if direction_pred == 1 else 'DOWN'}\")\n",
        "        if direction_prob is not None:\n",
        "            print(f\"Probability - Down: {direction_prob[0]:.3f}, Up: {direction_prob[1]:.3f}\")\n",
        "        print(f\"Next day return prediction: {return_pred:.4f} ({return_pred*100:.2f}%)\")\n",
        "\n",
        "        return direction_pred, return_pred, direction_prob\n",
        "\n",
        "    def run_full_analysis(self):\n",
        "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
        "        print(\"Starting Stock Market Prediction Analysis...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Prepare data\n",
        "        self.prepare_data()\n",
        "\n",
        "        # # Visualize data\n",
        "        # self.visualize_data()\n",
        "\n",
        "        # Prepare features\n",
        "        X, y_reg, y_clf, feature_columns = self.prepare_features()\n",
        "        self.feature_columns = feature_columns\n",
        "\n",
        "        # print(f\"Dataset shape: {X.shape}\")\n",
        "        # print(f\"Features: {len(feature_columns)}\")\n",
        "\n",
        "        # Train models\n",
        "        X_test_clf, y_test_clf, y_pred_clf = self.train_classification_model(X, y_clf)\n",
        "        X_test_reg, y_test_reg, y_pred_reg = self.train_regression_model(X, y_reg)\n",
        "\n",
        "        # Feature importance\n",
        "        self.feature_importance(feature_columns)\n",
        "\n",
        "        # Make predictions\n",
        "        self.make_predictions()\n",
        "\n",
        "        print(\"\\nAnalysis completed successfully!\")\n",
        "\n",
        "        return self.models, self.data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Demo"
      ],
      "metadata": {
        "id": "eA4yMO2W_3jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the predictor\n",
        "    fred_api_key = \"4e0ac07a1fa873e5599d209f5a3465b9\"\n",
        "\n",
        "    predictor = StockMarketPredictor(\n",
        "        fred_api_key=fred_api_key,\n",
        "        stock_symbol='SPY',  # S&P 500 ETF\n",
        "        start_date='2015-01-01'\n",
        "    )\n",
        "    models, data = predictor.run_full_analysis()\n",
        "\n",
        "    clf, reg = models[\"classification\"][\"model\"], models[\"classification\"][\"model\"]\n",
        "\n",
        "    joblib.dump(clf, \"clf_model.sav\")\n",
        "    joblib.dump(reg, \"reg_model.sav\")\n",
        "\n",
        "    # # You can access the trained models and data\n",
        "    # print(f\"\\nClassification Model: {models['classification']['name']}\")\n",
        "    # print(f\"Classification Accuracy: {models['classification']['score']:.4f}\")\n",
        "    # print(f\"Regression Model: {models['regression']['name']}\")\n",
        "    # print(f\"Regression R²: {models['regression']['r2']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_CQF4EN_3FC",
        "outputId": "edee73dd-d980-4865-abd8-288926f7c585"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Stock Market Prediction Analysis...\n",
            "==================================================\n",
            "Preparing and cleaning data...\n",
            "Collecting stock data for SPY...\n",
            "Collecting macroeconomic data from FRED...\n",
            "  Fetching GDP...\n",
            "  Fetching UNEMPLOYMENT...\n",
            "  Fetching INFLATION...\n",
            "  Fetching FED_RATE...\n",
            "  Fetching VIX...\n",
            "  Fetching 10Y_TREASURY...\n",
            "  Fetching 3M_TREASURY...\n",
            "  Fetching CONSUMER_SENTIMENT...\n",
            "  Fetching INDUSTRIAL_PRODUCTION...\n",
            "  Fetching HOUSING_STARTS...\n",
            "  Fetching RETAIL_SALES...\n",
            "  Fetching M2_MONEY_SUPPLY...\n",
            "Final dataset shape: (2655, 52)\n",
            "Training classification models...\n",
            "Random Forest Accuracy: 0.5302\n",
            "Logistic Regression Accuracy: 0.5249\n",
            "\n",
            "Best Classification Model: Random Forest\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.69      0.50       224\n",
            "           1       0.51      0.24      0.32       307\n",
            "\n",
            "    accuracy                           0.43       531\n",
            "   macro avg       0.45      0.46      0.41       531\n",
            "weighted avg       0.46      0.43      0.40       531\n",
            "\n",
            "\n",
            "Training regression models...\n",
            "Random Forest - R²: -0.0330, MSE: 0.000114\n",
            "Linear Regression - R²: -0.1063, MSE: 0.000130\n",
            "\n",
            "Best Regression Model: Random Forest\n",
            "\n",
            "Feature Importance Analysis:\n",
            "\n",
            "Top 10 features for Random Forest (Classification):\n",
            "        feature  importance\n",
            "2   Log_Returns    0.083062\n",
            "1       Returns    0.078825\n",
            "30     VIX_lag7    0.075589\n",
            "6           RSI    0.074837\n",
            "5         MA_20    0.071307\n",
            "0        Volume    0.071025\n",
            "3    Volatility    0.069169\n",
            "29     VIX_lag1    0.067907\n",
            "4          MA_5    0.067621\n",
            "11          VIX    0.066419\n",
            "\n",
            "Top 10 features for Random Forest (Regression):\n",
            "        feature  importance\n",
            "11          VIX    0.140845\n",
            "29     VIX_lag1    0.135854\n",
            "30     VIX_lag7    0.117187\n",
            "1       Returns    0.109102\n",
            "0        Volume    0.106808\n",
            "2   Log_Returns    0.092536\n",
            "6           RSI    0.074138\n",
            "3    Volatility    0.042161\n",
            "4          MA_5    0.034742\n",
            "5         MA_20    0.034499\n",
            "\n",
            "Making predictions for the next 5 days...\n",
            "Next day direction prediction: DOWN\n",
            "Probability - Down: 0.522, Up: 0.478\n",
            "Next day return prediction: -0.0026 (-0.26%)\n",
            "\n",
            "Analysis completed successfully!\n"
          ]
        }
      ]
    }
  ]
}